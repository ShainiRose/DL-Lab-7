{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShainiRose/DL-Lab-7/blob/main/Gridworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qu7isMPEIjF"
      },
      "source": [
        "# The agent-environment interaction\n",
        "\n",
        "In this exercise, you will implement the interaction of a reinforecment learning agent with its environment. We will use the gridworld environment from the second lecture. You will find a description of the environment below, along with two pieces of relevant material from the lectures: the agent-environment interface and the Q-learning algorithm.\n",
        "\n",
        "1. Create an agent that chooses actions randomly with this environment.\n",
        "\n",
        "2. Create an agent that uses Q-learning. You can use initial Q values of 0, a stochasticity parameter for the $\\epsilon$-greedy policy function $\\epsilon=0.05$, and a learning rate $\\alpha = 0.1$. But feel free to experiment with other settings of these three parameters.\n",
        "\n",
        "3. Plot the mean total reward obtained by the two agents through the episodes. This is called a **learning curve**. Run enough episodes for the Q-learning agent to converge to a near-optimal policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNGrAPCaEIjG"
      },
      "source": [
        "## The agent-environment interface\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dkasthurirathna/dl/master/agent-environment.png\" style=\"width: 500px;\" align=\"left\"/>\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ-p0VgVEIjG"
      },
      "source": [
        "## The environment: Navigation in a gridworld\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dkasthurirathna/dl/master/gold.png\" style=\"width: 250px;\" align=\"left\"/>\n",
        "\n",
        "The agent has four possible actions in each state (grid square): west, north, south, and east. The actions are unreliable. They move the agent in the intended direction with probability 0.8, and with probability 0.2, they move the agent in a random other direction. It the direction of movement is blocked, the agent remains in the same grid square. The initial state of the agent is one of the five grid squares at the bottom, selected randomly. The grid squares with the gold and the bomb are **terminal states**. If the agent finds itself in one of these squares, the episode ends. Then a new episode begins with the agent at the initial state.\n",
        "\n",
        "You will use a reinforcement learning algorithm to compute the best policy for finding the gold with as few steps as possible while avoiding the bomb. For this, we will use the following reward function: $-1$ for each navigation action, an additional $+10$ for finding the gold, and an additional $-10$ for hitting the bomb. For example, the immediate reward for transitioning into the square with the gold is $-1 + 10 = +9$. Do not use discounting (that is, set $\\gamma=1$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvw_VkX6EIjH"
      },
      "source": [
        "## Q-learning\n",
        "\n",
        "![title](https://raw.githubusercontent.com/dkasthurirathna/dl/master/q.png)\n",
        "From Sutton & Barto (1998), Reinforcement Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aj29nt13EIjH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHWOrZ_1EIjL"
      },
      "source": [
        "# Classes for the Enviroment and the Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WdlTPDVEIjM"
      },
      "source": [
        "- The GridWorld class contains the environment\n",
        "- The dimensions of the environment are defined\n",
        "- Locations of all rewards are stored\n",
        "- Functions for different methods written\n",
        "    - `get_available_actions` returns possible actions\n",
        "    - `agent_on_map` prints out current location of the agent on the grid (used for debugging)\n",
        "    - `get_reward` returns the reward for an input position\n",
        "    - `make_step` moves the agent in a specified direction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vo67tnGZEIjM"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    ## Initialise starting data\n",
        "    def __init__(self):\n",
        "        # Set information about the gridworld\n",
        "        self.height = 8\n",
        "        self.width = 8\n",
        "        self.grid = np.zeros(( self.height, self.width)) - 1\n",
        "\n",
        "        # Set random start location for the agent\n",
        "        self.current_location = ( 4, np.random.randint(0,5))\n",
        "\n",
        "        # Set locations for the bomb and the gold\n",
        "        self.bomb_location = (1,3)\n",
        "        self.gold_location = (0,3)\n",
        "        self.terminal_states = [ self.bomb_location, self.gold_location]\n",
        "\n",
        "        # Set grid rewards for special cells\n",
        "        self.grid[ self.bomb_location[0], self.bomb_location[1]] = -10\n",
        "        self.grid[ self.gold_location[0], self.gold_location[1]] = 10\n",
        "\n",
        "        # Set available actions\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    # ✅ NEW: reset method (needed for play loop)\n",
        "    def reset(self):\n",
        "        self.current_location = (4, np.random.randint(0,5))  # random start\n",
        "        return self.current_location\n",
        "\n",
        "    # ✅ NEW: step method (wrapper around make_step)\n",
        "    def step(self, action):\n",
        "        reward = self.make_step(action)\n",
        "        done = self.check_state() == 'TERMINAL'\n",
        "        return self.current_location, reward, done\n",
        "\n",
        "    # ✅ NEW: agent_pos property (for compatibility with Q_Agent)\n",
        "    @property\n",
        "    def agent_pos(self):\n",
        "        return self.current_location\n",
        "\n",
        "    ## Your existing methods (unchanged) ⬇⬇⬇\n",
        "\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Returns possible actions\"\"\"\n",
        "        return self.actions\n",
        "\n",
        "    def agent_on_map(self):\n",
        "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
        "        grid = np.zeros(( self.height, self.width))\n",
        "        grid[ self.current_location[0], self.current_location[1]] = 1\n",
        "        return grid\n",
        "\n",
        "    def get_reward(self, new_location):\n",
        "        \"\"\"Returns the reward for an input position\"\"\"\n",
        "        return self.grid[ new_location[0], new_location[1]]\n",
        "\n",
        "    def make_step(self, action):\n",
        "        \"\"\"Moves the agent in the specified direction. If agent is at a border, agent stays still\n",
        "        but takes negative reward. Function returns the reward for the move.\"\"\"\n",
        "        last_location = self.current_location\n",
        "\n",
        "        # UP\n",
        "        if action == 'UP':\n",
        "            if last_location[0] == 0:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0] - 1, self.current_location[1])\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        # DOWN\n",
        "        elif action == 'DOWN':\n",
        "            if last_location[0] == self.height - 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0] + 1, self.current_location[1])\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        # LEFT\n",
        "        elif action == 'LEFT':\n",
        "            if last_location[1] == 0:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0], self.current_location[1] - 1)\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        # RIGHT\n",
        "        elif action == 'RIGHT':\n",
        "            if last_location[1] == self.width - 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0], self.current_location[1] + 1)\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def check_state(self):\n",
        "        \"\"\"Check if the agent is in a terminal state (gold or bomb), if so return 'TERMINAL'\"\"\"\n",
        "        if self.current_location in self.terminal_states:\n",
        "            return 'TERMINAL'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "j5lVvvTaEIjP"
      },
      "outputs": [],
      "source": [
        "class RandomAgent():\n",
        "    # Choose a random action\n",
        "    def choose_action(self, available_actions):\n",
        "        \"\"\"Returns a random choice of the available actions\"\"\"\n",
        "        return np.random.choice(available_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P3A4wx5DEIjR"
      },
      "outputs": [],
      "source": [
        "class Q_Agent():\n",
        "    # Intialise\n",
        "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1):\n",
        "        self.environment = environment\n",
        "        self.q_table = dict() # Store all Q-values in dictionary of dictionaries\n",
        "        for x in range(environment.height): # Loop through all possible grid spaces, create sub-dictionary for each\n",
        "            for y in range(environment.width):\n",
        "                self.q_table[(x,y)] = {'UP':0, 'DOWN':0, 'LEFT':0, 'RIGHT':0} # Populate sub-dictionary with zero values for possible moves\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def choose_action(self, available_actions, state):   # <<< CHANGE HERE: added state argument\n",
        "        \"\"\"Returns the optimal action from Q-Value table. If multiple optimal actions, chooses random choice.\n",
        "        Will make an exploratory random action dependent on epsilon.\"\"\"\n",
        "        import random\n",
        "        if random.uniform(0,1) < self.epsilon:\n",
        "            action = random.choice(available_actions)\n",
        "        else:\n",
        "            # <<< CHANGE HERE: replaced self.environment.agent_pos with state >>>\n",
        "            max_q = max([self.q_table[state][a] for a in available_actions])\n",
        "            best_actions = [a for a in available_actions if self.q_table[state][a] == max_q]\n",
        "            action = random.choice(best_actions)\n",
        "        return action\n",
        "\n",
        "    def learn(self, old_state, reward, new_state, action):\n",
        "        \"\"\"Updates the Q-value table using Q-learning\"\"\"\n",
        "        old_q = self.q_table[old_state][action]\n",
        "        max_future_q = max(self.q_table[new_state].values())\n",
        "        new_q = old_q + self.alpha * (reward + self.gamma * max_future_q - old_q)\n",
        "        self.q_table[old_state][action] = new_q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7YfXnEmTEIjT"
      },
      "outputs": [],
      "source": [
        "def play(environment, agent, trials=500, learn=False):\n",
        "    rewards = []\n",
        "    for _ in range(trials):\n",
        "        state = environment.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            available_actions = environment.get_valid_actions(state)\n",
        "            action = agent.choose_action(available_actions, state)   # <<< CHANGE HERE\n",
        "            new_state, reward, done = environment.step(action)\n",
        "\n",
        "            if learn:\n",
        "                agent.learn(state, reward, new_state, action)\n",
        "\n",
        "            state = new_state\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8vRB7QcEIjV"
      },
      "source": [
        "## Run Random Agent\n",
        "\n",
        "- Random agent moves randomly and does not learn from it's actions.\n",
        "- This gives a base performance to compare the Q-Learning agent to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzqYMGCSEIjW",
        "outputId": "781cf4f8-db27-413c-c9c1-8878ed946567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current position of the agent = (4, 3)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Available_actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
            "Randomly chosen action = RIGHT\n",
            "Reward obtained = -1.0\n",
            "Current position of the agent = (4, 4)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = RandomAgent()\n",
        "\n",
        "print(\"Current position of the agent =\", env.current_location)\n",
        "print(env.agent_on_map())\n",
        "available_actions = env.get_available_actions()\n",
        "print(\"Available_actions =\", available_actions)\n",
        "chosen_action = agent.choose_action(available_actions)\n",
        "print(\"Randomly chosen action =\", chosen_action)\n",
        "reward = env.make_step(chosen_action)\n",
        "print(\"Reward obtained =\", reward)\n",
        "print(\"Current position of the agent =\", env.current_location)\n",
        "print(env.agent_on_map())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re_kTTy1EIja"
      },
      "source": [
        "- Here the random agent is ran for 500 trials\n",
        "- Performance is obviously inconsistent and not optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Op2gsYAtEIjb",
        "outputId": "c38829b8-0db8-49fd-efbe-c4208a0f849a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GridWorld' object has no attribute 'get_valid_actions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1958486122.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrandom_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreward_per_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Simple learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-923751000.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(environment, agent, trials, learn)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mavailable_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_valid_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# <<< CHANGE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridWorld' object has no attribute 'get_valid_actions'"
          ]
        }
      ],
      "source": [
        "# Initialize environment and agent\n",
        "environment = GridWorld()\n",
        "random_agent = RandomAgent()\n",
        "\n",
        "reward_per_episode = play(environment, random_agent, trials=500)\n",
        "\n",
        "# Simple learning curve\n",
        "plt.plot(reward_per_episode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQhwz2OSEIjd"
      },
      "source": [
        "## Q-Agent\n",
        "\n",
        "- Here the Q-Learning agent is ran for 500 trials again\n",
        "- Performance is plotted\n",
        "- Performance increases greatly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "uZD-EiQkEIje",
        "outputId": "4002a783-a7d1-4580-aac1-e625cc80bea9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GridWorld' object has no attribute 'get_valid_actions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-876111990.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note the learn=True argument!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreward_per_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magentQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Simple learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-923751000.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(environment, agent, trials, learn)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mavailable_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_valid_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# <<< CHANGE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridWorld' object has no attribute 'get_valid_actions'"
          ]
        }
      ],
      "source": [
        "environment = GridWorld()\n",
        "agentQ = Q_Agent(environment)\n",
        "\n",
        "# Note the learn=True argument!\n",
        "reward_per_episode = play(environment, agentQ, trials=500, learn=True)\n",
        "\n",
        "# Simple learning curve\n",
        "plt.plot(reward_per_episode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVVO8cY3EIjh"
      },
      "source": [
        "Print the final Q-value table with nice formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ1eVXMMEIjh",
        "outputId": "79d8089c-f254-4940-95dd-86a973066935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(0, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(0, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(0, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(0, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(0, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(0, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(0, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(1, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(2, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(3, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(4, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(5, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(6, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 0)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 1)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 2)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 3)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 4)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 5)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 6)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n",
            "(7, 7)\n",
            "\tUP\n",
            "\t\t0\n",
            "\tDOWN\n",
            "\t\t0\n",
            "\tLEFT\n",
            "\t\t0\n",
            "\tRIGHT\n",
            "\t\t0\n"
          ]
        }
      ],
      "source": [
        "def pretty(d, indent=0):\n",
        "    for key, value in d.items():\n",
        "        print('\\t' * indent + str(key))\n",
        "        if isinstance(value, dict):\n",
        "            pretty(value, indent+1)\n",
        "        else:\n",
        "            print('\\t' * (indent+1) + str(value))\n",
        "\n",
        "\n",
        "pretty(agentQ.q_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBGERbMKEIjk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}